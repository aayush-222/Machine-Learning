# **Case Study : Automated Fake / Inaccurate News Identification using Machine Learning Techniques.**



##### **Author (S) :** Aayush Kumar | Abeer Pal | Akhil Raj | Om Tripathi | Md Tausif | Vaibhav Jain | Sahil Kumar

##### **Admission (No.) :** 23SCSE1011573 | 23SCSE1012032 | 23SCSE1012367 | 23SCSE1010179 | 23SCSE1012492 | 23SCSE1011050 | 23SCSE1010505

##**Problem Type :**
Supervised Machine Learning - Classification

##**Objective :**
To classify online news articles as Fake or Real using TF-IDF feature extraction and machine learning algorithms.

##**Algorithms Used:**
      1. Naive Bayes
      2. Logistic Regression

##**Importing Required Libraries :**

# Data handling
import pandas as pd
import numpy as np

# Text preprocessing
import re
import string

# Machine Learning utilities
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Models
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression

# Evaluation metrics
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_auc_score, roc_curve
)

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Feature combination
from scipy.sparse import hstack


##**Loading the Dataset :**

df = pd.read_csv("fake_news_dataset.csv")

print("Dataset Shape:", df.shape)
df.head()


##**Exploring Dataset Information :**

# Check column information
df.info()

# Check class distribution
df['label'].value_counts()

# Check missing values
df.isnull().sum()


##**Text Cleaning Function :**

def clean_text(text):
    text = text.lower()
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', ' ', text)
    return text

# Apply cleaning
df['clean_text'] = df['text'].apply(clean_text)


##**Separating Input Features and Target :**

# Text feature
X_text = df['clean_text']

# Numerical features (explicit 3-input-feature requirement)
X_numeric = df[['word_count', 'text_length']]

# Target variable
y = df['label']


##**Splitting Data into Train - Validation - Test Sets :**

X_text_train, X_text_temp, X_num_train, X_num_temp, y_train, y_temp = train_test_split(
    X_text, X_numeric, y,
    test_size=0.30,
    random_state=42,
    stratify=y
)

X_text_val, X_text_test, X_num_val, X_num_test, y_val, y_test = train_test_split(
    X_text_temp, X_num_temp, y_temp,
    test_size=0.50,
    random_state=42,
    stratify=y_temp
)

print("Training size:", X_text_train.shape[0])
print("Validation size:", X_text_val.shape[0])
print("Test size:", X_text_test.shape[0])


##**Converting Text Data into Numerical Features using TF-IDF :**

tfidf = TfidfVectorizer(max_features=5000, stop_words='english')

X_train_tfidf = tfidf.fit_transform(X_text_train)
X_val_tfidf = tfidf.transform(X_text_val)
X_test_tfidf = tfidf.transform(X_text_test)

# Combine text features with numeric features
X_train_final = hstack([X_train_tfidf, X_num_train])
X_val_final = hstack([X_val_tfidf, X_num_val])
X_test_final = hstack([X_test_tfidf, X_num_test])


##**Training Naive Bayes Classifier :**

nb_model = MultinomialNB()
nb_model.fit(X_train_final, y_train)

nb_pred = nb_model.predict(X_test_final)
nb_prob = nb_model.predict_proba(X_test_final)[:, 1]


##**Training Logistic Regression Classifier :**

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_final, y_train)

lr_pred = lr_model.predict(X_test_final)
lr_prob = lr_model.predict_proba(X_test_final)[:, 1]


##**Function to Evaluate Model Performance :**

def evaluate_model(name, y_true, y_pred, y_prob):
    print(f"\n{name} Performance")
    print("Accuracy :", accuracy_score(y_true, y_pred))
    print("Precision:", precision_score(y_true, y_pred))
    print("Recall   :", recall_score(y_true, y_pred))
    print("F1 Score :", f1_score(y_true, y_pred))
    print("ROC-AUC  :", roc_auc_score(y_true, y_prob))

evaluate_model("Naive Bayes", y_test, nb_pred, nb_prob)
evaluate_model("Logistic Regression", y_test, lr_pred, lr_prob)


##**Confusion Matrix Plot :**

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

sns.heatmap(confusion_matrix(y_test, nb_pred),
            annot=True, fmt='d', ax=axes[0])
axes[0].set_title("Naive Bayes Confusion Matrix")

sns.heatmap(confusion_matrix(y_test, lr_pred),
            annot=True, fmt='d', ax=axes[1])
axes[1].set_title("Logistic Regression Confusion Matrix")

plt.show()


##**Graph Outputs :**

# Store metric values for both models
metrics = {
    "Accuracy": [
        accuracy_score(y_test, nb_pred),
        accuracy_score(y_test, lr_pred)
    ],
    "Precision": [
        precision_score(y_test, nb_pred),
        precision_score(y_test, lr_pred)
    ],
    "Recall": [
        recall_score(y_test, nb_pred),
        recall_score(y_test, lr_pred)
    ],
    "F1 Score": [
        f1_score(y_test, nb_pred),
        f1_score(y_test, lr_pred)
    ]
}

# Convert to DataFrame for plotting
metrics_df = pd.DataFrame(metrics, index=["Naive Bayes", "Logistic Regression"])

# Plot bar chart
metrics_df.plot(kind='bar', figsize=(10, 5))
plt.title("Performance Comparison of Machine Learning Models")
plt.ylabel("Score")
plt.xlabel("Model")
plt.ylim(0, 1)
plt.xticks(rotation=0)
plt.legend(loc='lower right')
plt.grid(axis='y')

plt.show()


##**ROC Curve comparison :**

nb_fpr, nb_tpr, _ = roc_curve(y_test, nb_prob)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_prob)

plt.figure(figsize=(6, 5))
plt.plot(nb_fpr, nb_tpr, label='Naive Bayes')
plt.plot(lr_fpr, lr_tpr, label='Logistic Regression')
plt.plot([0, 1], [0, 1], linestyle='--')

plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve Comparison")
plt.legend()
plt.show()


##**Comparing Both Models :**

comparison = pd.DataFrame({
    "Model": ["Naive Bayes", "Logistic Regression"],
    "Accuracy": [
        accuracy_score(y_test, nb_pred),
        accuracy_score(y_test, lr_pred)
    ],
    "Precision": [
        precision_score(y_test, nb_pred),
        precision_score(y_test, lr_pred)
    ],
    "Recall": [
        recall_score(y_test, nb_pred),
        recall_score(y_test, lr_pred)
    ],
    "F1 Score": [
        f1_score(y_test, nb_pred),
        f1_score(y_test, lr_pred)
    ],
    "ROC-AUC": [
        roc_auc_score(y_test, nb_prob),
        roc_auc_score(y_test, lr_prob)
    ]
})

comparison
